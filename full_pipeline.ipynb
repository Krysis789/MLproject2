{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT USED PACKAGES AND SET SEED\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import *\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pbs\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "from IPython.display import display\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.strftime(datetime.now(), \"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate(input_filename):\n",
    "    with open(\"Datasets/\"+input_filename, encoding=\"utf-8\") as f :\n",
    "        tweets = f.read().splitlines()\n",
    "        df_tweets = pd.DataFrame(tweets,columns=['Tweets'])\n",
    "        df_tweets.drop_duplicates(inplace=True)\n",
    "    with open('Processed_data/no_dupl_' + input_filename, \"w+\", encoding = \"UTF-8\") as f:\n",
    "        for tweet in df_tweets[\"Tweets\"]:\n",
    "            f.write(\"%s\\n\" % tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ['train_pos.txt','train_neg.txt','train_neg_full.txt','train_pos_full.txt']:\n",
    "    drop_duplicate(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates test\n",
    "with open(\"Datasets/test_data.txt\", encoding=\"utf-8\") as f :\n",
    "    test = f.read().splitlines()\n",
    "    tweets = []\n",
    "    ids = []\n",
    "    for line in test:\n",
    "        id, tweet = line.split(',',1)\n",
    "        tweets.append(tweet)\n",
    "        ids.append(id)\n",
    "    df_test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    df_test.drop_duplicates(inplace=True)\n",
    "    \n",
    "with open('Processed_data/no_dupl_test_data.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in df_test[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate all text files (pos, neg and test) to create a dataset used as reference for twitter language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Processed_data/twitter_language.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    for path in ['Processed_data/no_dupl_train_pos.txt','Processed_data/no_dupl_train_neg.txt', 'Processed_data/no_dupl_test_data.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Processed_data/twitter_language_full.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    for path in ['Processed_data/no_dupl_train_pos_full.txt','Processed_data/no_dupl_train_neg_full.txt', 'Processed_data/no_dupl_test_data.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a frequency dictionnary of words occuring at least *threshold* times in the dataset, \n",
    "## that will be used as a reference for spell checking\n",
    "\n",
    "output_filename = 'Processed_data/vocab_freq.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/twitter_language.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a frequency dictionnary of words occuring at least *threshold* times in the dataset, \n",
    "## that will be used as a reference for spell checking\n",
    "\n",
    "output_filename = 'Processed_data/vocab_freq_full.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/twitter_language_full.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionnary(input_vocab, threshold):\n",
    "    full_vocab = pd.read_csv(input_vocab+\".txt\", sep = \"\\s+\", header=None, engine='python')\n",
    "    cut_vocab = full_vocab[full_vocab[0] >= threshold]\n",
    "    cut_vocab.columns = [\"number occ\",\"word\"]\n",
    "    cut_vocab.set_index(\"word\",inplace=True)\n",
    "    with open(input_vocab + '.json', 'w') as f:\n",
    "        json.dump(cut_vocab[\"number occ\"].to_dict(), f) \n",
    "\n",
    "build_dictionnary(\"Processed_data/vocab_freq\", threshold=5) #TODO try other threshold\n",
    "build_dictionnary(\"Processed_data/vocab_freq_full\", threshold=5) #TODO try other threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spell correction functions\n",
    "def correct(tweet, dict_corr):\n",
    "    list_words = tweet.split()\n",
    "    for i, word in enumerate(list_words):\n",
    "        if word in dict_corr :\n",
    "            list_words[i] = dict_corr[word]\n",
    "    corr_tweet = ' '.join(list_words)\n",
    "    return (corr_tweet) \n",
    "\n",
    "def spell_correction(data):\n",
    "    spell = SpellChecker(distance=1) # TODO if possible, try distance=2\n",
    "    spell.word_frequency.load_dictionary('Processed_data/vocab_freq_full.json') #'Processed_data/vocab_freq_full.json')\n",
    "    dict_corr = {}\n",
    "    for tweet in data[\"Tweets\"]:\n",
    "        list_words = tweet.split()\n",
    "        for i, word in enumerate(list_words):\n",
    "            if word not in dict_corr :\n",
    "                if word in spell.unknown([word]):\n",
    "                    dict_corr[word] = spell.correction(word)\n",
    "    data[\"Tweets\"] = data[\"Tweets\"].apply(lambda x : correct(x, dict_corr))\n",
    "    return data, dict_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the used datasets here\n",
    "positive_dataset = 'Processed_data/no_dupl_train_pos_full.txt' #'Processed_data/no_dupl_train_pos_full.txt'\n",
    "negative_dataset = 'Processed_data/no_dupl_train_neg_full.txt' #'Processed_data/no_dupl_train_neg_full.txt'\n",
    "\n",
    "def load_in_pd(data_path):\n",
    "    with open(data_path, encoding=\"utf-8\") as f :\n",
    "        x = f.read().splitlines()\n",
    "        x = pd.DataFrame(x,columns=['Tweets'])\n",
    "    return (x)\n",
    "\n",
    "positive_pd = load_in_pd(positive_dataset)\n",
    "negative_pd = load_in_pd(negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done positive\n",
      "Wall time: 12min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### APPLYING spell correction to pos and neg\n",
    "\n",
    "positive_preprocessed, dict_corr1 = spell_correction(positive_pd)  \n",
    "with open('Processed_data/' + 'positive_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f: #positive_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in positive_preprocessed[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)\n",
    "del positive_preprocessed\n",
    "print(\"done positive\")\n",
    "\n",
    "negative_preprocessed, dict_corr2 = spell_correction(negative_pd)\n",
    "with open('Processed_data/' + 'negative_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f: #negative_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in negative_preprocessed[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)\n",
    "del negative_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### APPLYING spell correction to test\n",
    "\n",
    "test_dataset = \"Datasets/test_data.txt\"\n",
    "\n",
    "def load_in_pd_test(data_path):\n",
    "    with open(data_path, encoding=\"utf-8\") as f :\n",
    "        test = f.read().splitlines()\n",
    "        tweets = []\n",
    "        ids = []\n",
    "        for line in test:\n",
    "            id, tweet = line.split(',',1)\n",
    "            tweets.append(tweet)\n",
    "            ids.append(id)\n",
    "        test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    return test\n",
    "\n",
    "test_pd = load_in_pd_test(test_dataset)\n",
    "test_spell, dict_corr_test = spell_correction(test_pd) \n",
    "\n",
    "with open(\"Processed_data/test_spell.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
    "    for index, row in test_spell.iterrows():\n",
    "        f.write(index + \",\" + row[\"Tweets\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather input for word embeddings\n",
    "with open(\"Processed_data/input_WE_full.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    with open(\"Processed_data/test_spell.txt\",encoding=\"utf-8\") as test:\n",
    "        test = test.readlines()\n",
    "        for line in test:\n",
    "            id, tweet = line.split(',',1)\n",
    "            f.write(\"%s\" % tweet)\n",
    "    \n",
    "    for path in ['Processed_data/positive_spell_full.txt','Processed_data/negative_spell_full.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'Processed_data/vocab_freq_after_spell_full.txt' #'Processed_data/vocab_freq_after_spell_full.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/input_WE_full.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab_spell = pd.read_csv('Processed_data/vocab_freq_after_spell_full.txt', sep = \"\\s+\", header=None, engine='python')\n",
    "full_vocab_spell.columns = [\"number occ\",\"word\"]\n",
    "full_vocab_spell.set_index(\"word\",inplace=True)\n",
    "    #with open(input_vocab + '.json', 'w') as f:\n",
    "     #   json.dump(cut_vocab[\"number occ\"].to_dict(), f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.yscale(\"log\")\n",
    "plt.hist(full_vocab_spell[\"number occ\"],bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number occ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>*</th>\n",
       "      <td>50136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>50779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>51981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>52740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;3</th>\n",
       "      <td>54287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can't</th>\n",
       "      <td>54573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>54748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thanks</th>\n",
       "      <td>54976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>im</th>\n",
       "      <td>56037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>56119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haha</th>\n",
       "      <td>57184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>57514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>its</th>\n",
       "      <td>58869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>59789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>60257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>60530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how</th>\n",
       "      <td>61053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>61109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>back</th>\n",
       "      <td>62317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'</th>\n",
       "      <td>62360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it's</th>\n",
       "      <td>65153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>65954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>67269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please</th>\n",
       "      <td>68886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>69243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>follow</th>\n",
       "      <td>69995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>too</th>\n",
       "      <td>72692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>74903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>75347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>164482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>164889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>175254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>183886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>187726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>188008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>189213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>199259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"</th>\n",
       "      <td>229697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>264701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>271257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>274428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>282043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>303497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?</th>\n",
       "      <td>310627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>319236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>381306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>417371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;url&gt;</th>\n",
       "      <td>444390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>454663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>472888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>496427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>538340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>649292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>666559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>684704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>687291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>940049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>945602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt;</th>\n",
       "      <td>1514471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        number occ\n",
       "word              \n",
       "*            50136\n",
       "as           50779\n",
       "think        51981\n",
       "he           52740\n",
       "2            53311\n",
       "<3           54287\n",
       "can't        54573\n",
       "going        54748\n",
       "thanks       54976\n",
       "im           56037\n",
       "today        56119\n",
       "haha         57184\n",
       "x            57514\n",
       "its          58869\n",
       "time         59789\n",
       "really       60257\n",
       "about        60530\n",
       "how          61053\n",
       "got          61109\n",
       "back         62317\n",
       "'            62360\n",
       "it's         65153\n",
       "want         65954\n",
       "see          67269\n",
       "please       68886\n",
       "from         69243\n",
       "follow       69995\n",
       "too          72692\n",
       "day          74903\n",
       "will         75347\n",
       "...            ...\n",
       ")           164482\n",
       "be          164889\n",
       "-           175254\n",
       "with        183886\n",
       "that        187726\n",
       "so          188008\n",
       "on          189213\n",
       "this        199259\n",
       "\"           229697\n",
       "it          264701\n",
       "in          271257\n",
       "for         274428\n",
       "is          282043\n",
       "of          303497\n",
       "?           310627\n",
       "me          319236\n",
       "my          381306\n",
       "and         417371\n",
       "<url>       444390\n",
       "...         454663\n",
       "a           472888\n",
       "(           496427\n",
       "you         538340\n",
       "to          649292\n",
       ",           666559\n",
       "the         684704\n",
       ".           687291\n",
       "i           940049\n",
       "!           945602\n",
       "<user>     1514471\n",
       "\n",
       "[96 rows x 1 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_vocab_spell[full_vocab_spell[\"number occ\"]>50000].sort_values(by=\"number occ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##APPLY build_vocab.sh AND CUT THE VOCABULARY USING THE CHOSEN THRESHOLD\n",
    "\n",
    "#Choose the desired cutting parameter here (Tokens with >= cut_threshold occurrences are kept)\n",
    "cut_threshold = 5\n",
    "\n",
    "output_filename = 'Processed_data/vocab_full' + time + '.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/input_WE_full.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")\n",
    "    \n",
    "def cut_and_save_vocab(file_in, file_out):\n",
    "    full_vocab = pd.read_csv(file_in, sep = \"(\\s+)\", header=None, engine = 'python')\n",
    "    cutted_vocab = full_vocab[full_vocab[0] >= cut_threshold][2]\n",
    "    with open(file_out, 'w+') as f:\n",
    "        f.write(cutted_vocab.to_string(header = False, index = False))\n",
    "    \n",
    "cut_and_save_vocab('Processed_data/vocab_full' + time + '.txt', 'Processed_data/vocab_cut_full' + time + '.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DUMP THE BUILT VOCABULARY TO A PICKLE FILE\n",
    "vocab = dict()\n",
    "with open('Processed_data/vocab_cut_full' + time + '.txt') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        vocab[line.strip()] = idx\n",
    "\n",
    "with open('Processed_data/vocab_full' + time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "47322680\n",
      "summing duplicates (this can take a while)\n",
      "7324683\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "54631410\n",
      "summing duplicates (this can take a while)\n",
      "11762272\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "59009757\n",
      "summing duplicates (this can take a while)\n",
      "15359122\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "60511431\n",
      "summing duplicates (this can take a while)\n",
      "18355583\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "54190968\n",
      "summing duplicates (this can take a while)\n",
      "20357878\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "56101021\n",
      "summing duplicates (this can take a while)\n",
      "22109942\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "57985493\n",
      "summing duplicates (this can take a while)\n",
      "23715145\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "2190000\n",
      "2200000\n",
      "2210000\n",
      "2220000\n",
      "2230000\n",
      "2240000\n",
      "2250000\n",
      "2260000\n",
      "2270000\n",
      "44092826\n",
      "summing duplicates (this can take a while)\n",
      "Wall time: 9min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##CREATE A CO-OCCURRENCE MATRIX\n",
    "def create_cooc(vocab_file, negative_file, positive_file, output_file):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    vocab_size = len(vocab)\n",
    "    data, row, col = [], [], []\n",
    "    counter = 1\n",
    "    for fn in [negative_file, positive_file]:\n",
    "        with open(fn,encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "                tokens = [t for t in tokens if t >= 0]\n",
    "                for i,t1 in enumerate(tokens):\n",
    "                    for t2 in tokens[i+1:]:\n",
    "                        if t1==t2 :\n",
    "                            data.append(1/2)\n",
    "                        else :\n",
    "                            data.append(1)\n",
    "                        row.append(min(t1,t2))\n",
    "                        col.append(max(t1,t2))\n",
    "\n",
    "                if counter % 10000 == 0:\n",
    "                    print(counter)\n",
    "                counter += 1\n",
    "\n",
    "                if counter % 300000 == 0:\n",
    "                    print(len(data))\n",
    "                    cooc = coo_matrix((data, (row, col)))\n",
    "                    print(\"summing duplicates (this can take a while)\")\n",
    "                    cooc.sum_duplicates()\n",
    "                    data=list(cooc.data)\n",
    "                    row=list(cooc.row)\n",
    "                    col=list(cooc.col)\n",
    "                    print(len(data))\n",
    "\n",
    "    print(len(data))\n",
    "    cooc = coo_matrix((data, (row, col)))\n",
    "    print(\"summing duplicates (this can take a while)\")\n",
    "    cooc.sum_duplicates()\n",
    "    \n",
    "    data=list(cooc.data)\n",
    "    row=list(cooc.row)\n",
    "    col=list(cooc.col)\n",
    "    final_row = row+col\n",
    "    final_col = col+row\n",
    "    data = data+data\n",
    "    cooc = coo_matrix((data, (final_row, final_col)))\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(cooc, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "#Used datasets should be defined on the second code block\n",
    "create_cooc('Processed_data/vocab_full' + time + '.pkl', 'Processed_data/negative_spell_full.txt', 'Processed_data/positive_spell_full.txt',\n",
    "            'Processed_data/cooc_pickle_full' + time + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "2190000\n",
      "2200000\n",
      "2210000\n",
      "2220000\n",
      "2230000\n",
      "2240000\n",
      "2250000\n",
      "2260000\n",
      "2270000\n",
      "58865400\n",
      "summing duplicates (this can take a while)\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##CREATE A CO-OCCURRENCE MATRIX with context defined with threshold distance\n",
    "def context(lst,index,threshold):\n",
    "    return(lst[index+1 : min(index+threshold,len(lst)-1)])\n",
    "\n",
    "def create_cooc_dist(vocab_file, negative_file, positive_file, output_file, threshold):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    vocab_size = len(vocab)\n",
    "    data, row, col = [], [], []\n",
    "    counter = 1\n",
    "    for fn in [negative_file, positive_file]:\n",
    "        with open(fn,encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "                #tokens = [t for t in tokens if t >= 0]\n",
    "                for i,t1 in enumerate(tokens):\n",
    "                    if t1 >= 0:\n",
    "                        for t2 in context(tokens,i,threshold):\n",
    "                            if t2 >= 0:\n",
    "                                if t1==t2 :\n",
    "                                    data.append(1/2)\n",
    "                                else :\n",
    "                                    data.append(1)\n",
    "                                row.append(min(t1,t2))\n",
    "                                col.append(max(t1,t2))\n",
    "\n",
    "                if counter % 10000 == 0:\n",
    "                    print(counter)\n",
    "                counter += 1\n",
    "\n",
    "                if counter % 3000000 == 0:\n",
    "                    print(len(data))\n",
    "                    cooc = coo_matrix((data, (row, col)))\n",
    "                    print(\"summing duplicates (this can take a while)\")\n",
    "                    cooc.sum_duplicates()\n",
    "                    data=list(cooc.data)\n",
    "                    row=list(cooc.row)\n",
    "                    col=list(cooc.col)\n",
    "                    print(len(data))\n",
    "\n",
    "    print(len(data))\n",
    "    cooc = coo_matrix((data, (row, col)))\n",
    "    print(\"summing duplicates (this can take a while)\")\n",
    "    cooc.sum_duplicates()\n",
    "    \n",
    "    data=list(cooc.data)\n",
    "    row=list(cooc.row)\n",
    "    col=list(cooc.col)\n",
    "    final_row = row+col\n",
    "    final_col = col+row\n",
    "    data = data+data\n",
    "    cooc = coo_matrix((data, (final_row, final_col)))\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(cooc, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "#Used datasets should be defined on the second code block\n",
    "create_cooc_dist('Processed_data/vocab_full' + time + '.pkl', 'Processed_data/negative_spell_full.txt', 'Processed_data/positive_spell_full.txt',\n",
    "            'Processed_data/cooc_dist_pickle_full' + time + '.pkl',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPLY glove.py\n",
    "random.seed(123)\n",
    "def glove(cooc_pickle, output_file):\n",
    "    print(\"loading cooccurrence matrix\")\n",
    "    with open(cooc_pickle, 'rb') as f:\n",
    "        cooc = pickle.load(f)\n",
    "    print(\"{} nonzero entries\".format(cooc.nnz))\n",
    "\n",
    "    nmax = 100\n",
    "    print(\"using nmax =\", nmax, \", cooc.max() =\", cooc.max())\n",
    "\n",
    "    print(\"initializing embeddings\")\n",
    "    embedding_dim = 20\n",
    "    xs = np.random.normal(size=(cooc.shape[0], embedding_dim))\n",
    "    ys = np.random.normal(size=(cooc.shape[1], embedding_dim))\n",
    "\n",
    "    eta = 0.001\n",
    "    alpha = 3 / 4\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
    "            logn = np.log(n)\n",
    "            fn = min(1.0, (n / nmax) ** alpha)\n",
    "            x, y = xs[ix, :], ys[jy, :]\n",
    "            scale = 2 * eta * fn * (logn - np.dot(x, y))\n",
    "            xs[ix, :] += scale * y\n",
    "            ys[jy, :] += scale * x\n",
    "    np.save(output_file, xs)\n",
    "\n",
    "glove('Processed_data/cooc_pickle' + time + '.pkl', 'Processed_data/embeddings' + time + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PREPARE DATA FOR TRAINING A CLASSIFIER\n",
    "\n",
    "def load_train_data(data_path):\n",
    "    \"\"\"Loads data and returns ids (event ids) and X (feature vector)\"\"\"\n",
    "    f = open(data_path, encoding=\"utf-8\")\n",
    "    x = f.read().splitlines()\n",
    "    x = pd.DataFrame(x, columns=['Tweets'])\n",
    "    return x\n",
    "\n",
    "def representation(tweet, we, vocab):\n",
    "    acc = np.array(0)\n",
    "    n_ignored_word = 0\n",
    "    for word in tweet.split():\n",
    "        if word not in vocab.keys():\n",
    "            n_ignored_word += 1\n",
    "        else:\n",
    "            try:\n",
    "                acc = np.add(acc,we[vocab[word]])\n",
    "            except: \n",
    "                #print(\"problem with \" + word) #last word from vocab is missing in cooc\n",
    "                n_ignored_word += 1\n",
    "    n = len(tweet.split()) - n_ignored_word\n",
    "    acc = acc / n\n",
    "    return(acc)\n",
    "\n",
    "\n",
    "def create_train_data(positive_path, negative_path, vocab, we):\n",
    "    pos = load_train_data(positive_path)\n",
    "    neg = load_train_data(negative_path)\n",
    "    pos[\"y\"] = 1\n",
    "    neg[\"y\"] = -1\n",
    "\n",
    "    print('pos...')\n",
    "    \n",
    "    pos.reset_index(drop = True, inplace = True)\n",
    "    pos[\"w\"] = pos[\"Tweets\"].apply(lambda x: representation(x, we, vocab))\n",
    "    pos.drop(\"Tweets\", axis=1, inplace = True)\n",
    "\n",
    "    col = [\"w\" + str(k) for k in range(np.shape(we)[1])]\n",
    "    pos[col] = pos[\"w\"].apply(pd.Series)\n",
    "    pos.drop(\"w\",axis=1,inplace=True)\n",
    "    \n",
    "    #remove the tweets which do not have any words used more than 5 times in the training dataset\n",
    "    pos.dropna(inplace=True) \n",
    "    \n",
    "    \n",
    "    with open('Processed_data/pos' + time + '.pkl', 'wb') as k:\n",
    "        pickle.dump(pos, k, pickle.HIGHEST_PROTOCOL)\n",
    "    del pos\n",
    "    print('pos done')\n",
    "    \n",
    "    print('neg')\n",
    "    neg.reset_index(drop = True, inplace = True)\n",
    "    neg[\"w\"] = neg[\"Tweets\"].apply(lambda x: representation(x, we, vocab))\n",
    "    neg.drop(\"Tweets\", axis=1, inplace = True)\n",
    "\n",
    "    col = [\"w\" + str(k) for k in range(np.shape(we)[1])]\n",
    "    neg[col] = neg[\"w\"].apply(pd.Series)\n",
    "    neg.drop(\"w\",axis=1,inplace=True)\n",
    "    \n",
    "    #remove the tweets which do not have any words used more than 5 times in the training dataset\n",
    "    neg.dropna(inplace=True)\n",
    "    \n",
    "    print('neg done')\n",
    "    \n",
    "    with open('Processed_data/pos' + time + '.pkl', 'rb') as j:\n",
    "        pos = pickle.load(j)\n",
    "    \n",
    "    print('lets try to concatenate')\n",
    "    train = pd.concat([pos, neg])\n",
    "    print('tadaaaa')\n",
    "    return train\n",
    "\n",
    "with open('Processed_data/vocab_' + time + '.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "we = np.load('Processed_data/embeddings' + time + '.npy')\n",
    "#The names of the datasets are defined in the second code block\n",
    "train = create_train_data(positive_dataset, negative_dataset, vocab, we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAIN A CLASSIFIER\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "X = train.drop(\"y\", axis=1)\n",
    "y = train[\"y\"]\n",
    "random.seed(123)\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD THE TEST DATA, USE THE MODEL TO PREDICT THE SENTIMENTS AND SAVE THE PREDICTIONS\n",
    "\n",
    "def load_and_prepare_test_data(data_path,vocab, we):\n",
    "    \"\"\"Loads data and returns ids (event ids) and X (feature vector)\"\"\"\n",
    "    f = open(data_path, encoding='utf-8')\n",
    "    test = f.read().splitlines()\n",
    "    tweets = []\n",
    "    ids = []\n",
    "    for line in test:\n",
    "        id, tweet = line.split(',',1)\n",
    "        tweets.append(tweet)\n",
    "        ids.append(id)\n",
    "    test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    test[\"w\"] = test[\"Tweets\"].apply(lambda x: representation(x, we, vocab))\n",
    "    col = [\"w\" + str(k) for k in range(np.shape(we)[1])]\n",
    "    test[col] = test[\"w\"].apply(pd.Series)\n",
    "    test.drop(\"w\", axis=1, inplace = True)\n",
    "    test.drop(\"Tweets\", axis=1, inplace = True)\n",
    "    \n",
    "    drops = test[test.isnull().any(axis=1)].index\n",
    "    #for the tweets which do not have any words in the cutted vocabulary, predict 1\n",
    "    test.fillna(1, inplace=True)\n",
    "    \n",
    "    return test,drops\n",
    "\n",
    "test, drops = load_and_prepare_test_data('Datasets/test_data.txt', vocab, we)\n",
    "test[\"Prediction\"] = clf.predict(test)\n",
    "#test[\"Id\"] = test.index\n",
    "test[\"Prediction\"].to_csv(\"Submissions/submission\" + time + \".csv\", header= True, index_label = \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
