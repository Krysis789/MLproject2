{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT USED PACKAGES AND SET SEED\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import *\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pbs\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "from IPython.display import display\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.strftime(datetime.now(), \"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate(input_filename):\n",
    "    with open(\"Datasets/\"+input_filename, encoding=\"utf-8\") as f :\n",
    "        tweets = f.read().splitlines()\n",
    "        df_tweets = pd.DataFrame(tweets,columns=['Tweets'])\n",
    "        df_tweets.drop_duplicates(inplace=True)\n",
    "    with open('Processed_data/no_dupl_' + input_filename, \"w+\", encoding = \"UTF-8\") as f:\n",
    "        for tweet in df_tweets[\"Tweets\"]:\n",
    "            f.write(\"%s\\n\" % tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ['train_pos.txt','train_neg.txt','train_neg_full.txt','train_pos_full.txt']:\n",
    "    drop_duplicate(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates test\n",
    "with open(\"Datasets/test_data.txt\", encoding=\"utf-8\") as f :\n",
    "    test = f.read().splitlines()\n",
    "    tweets = []\n",
    "    ids = []\n",
    "    for line in test:\n",
    "        id, tweet = line.split(',',1)\n",
    "        tweets.append(tweet)\n",
    "        ids.append(id)\n",
    "    df_test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    df_test.drop_duplicates(inplace=True)\n",
    "    \n",
    "with open('Processed_data/no_dupl_test_data.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in df_test[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate all text files (pos, neg and test) to create a dataset used as reference for twitter language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Processed_data/twitter_language.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    for path in ['Processed_data/no_dupl_train_pos.txt','Processed_data/no_dupl_train_neg.txt', 'Processed_data/no_dupl_test_data.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Processed_data/twitter_language_full.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    for path in ['Processed_data/no_dupl_train_pos_full.txt','Processed_data/no_dupl_train_neg_full.txt', 'Processed_data/no_dupl_test_data.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a frequency dictionnary of words occuring at least *threshold* times in the dataset, \n",
    "## that will be used as a reference for spell checking\n",
    "\n",
    "output_filename = 'Processed_data/vocab_freq.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/twitter_language.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a frequency dictionnary of words occuring at least *threshold* times in the dataset, \n",
    "## that will be used as a reference for spell checking\n",
    "\n",
    "output_filename = 'Processed_data/vocab_freq_full.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/twitter_language_full.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionnary(input_vocab, threshold):\n",
    "    full_vocab = pd.read_csv(input_vocab+\".txt\", sep = \"\\s+\", header=None, engine='python')\n",
    "    cut_vocab = full_vocab[full_vocab[0] >= threshold]\n",
    "    cut_vocab.columns = [\"number occ\",\"word\"]\n",
    "    cut_vocab.set_index(\"word\",inplace=True)\n",
    "    with open(input_vocab + '.json', 'w') as f:\n",
    "        json.dump(cut_vocab[\"number occ\"].to_dict(), f) \n",
    "\n",
    "build_dictionnary(\"Processed_data/vocab_freq\", threshold=5) #TODO try other threshold\n",
    "build_dictionnary(\"Processed_data/vocab_freq_full\", threshold=5) #TODO try other threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spell correction functions\n",
    "def correct(tweet, dict_corr):\n",
    "    list_words = tweet.split()\n",
    "    for i, word in enumerate(list_words):\n",
    "        if word in dict_corr :\n",
    "            list_words[i] = dict_corr[word]\n",
    "    corr_tweet = ' '.join(list_words)\n",
    "    return (corr_tweet) \n",
    "\n",
    "def spell_correction(data):\n",
    "    spell = SpellChecker(distance=1) # TODO if possible, try distance=2\n",
    "    spell.word_frequency.load_dictionary('Processed_data/vocab_freq.json') #'Processed_data/vocab_freq.json')\n",
    "    dict_corr = {}\n",
    "    for tweet in data[\"Tweets\"]:\n",
    "        list_words = tweet.split()\n",
    "        for i, word in enumerate(list_words):\n",
    "            if word not in dict_corr :\n",
    "                if word in spell.unknown([word]):\n",
    "                    dict_corr[word] = spell.correction(word)\n",
    "    data[\"Tweets\"] = data[\"Tweets\"].apply(lambda x : correct(x, dict_corr))\n",
    "    return data, dict_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the used datasets here\n",
    "positive_dataset = 'Processed_data/no_dupl_train_pos.txt' #'Processed_data/no_dupl_train_pos.txt'\n",
    "negative_dataset = 'Processed_data/no_dupl_train_neg.txt' #'Processed_data/no_dupl_train_neg.txt'\n",
    "\n",
    "def load_in_pd(data_path):\n",
    "    with open(data_path, encoding=\"utf-8\") as f :\n",
    "        x = f.read().splitlines()\n",
    "        x = pd.DataFrame(x,columns=['Tweets'])\n",
    "    return (x)\n",
    "\n",
    "positive_pd = load_in_pd(positive_dataset)\n",
    "negative_pd = load_in_pd(negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done positive\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### APPLYING DESIRED PREPROCESSING (to pos and neg)\n",
    "\n",
    "#Define the desired preprocessing method in this function\n",
    "def preprocess(tweet_data):\n",
    "    tweet_data, dict_corr = spell_correction(tweet_data)\n",
    "    return tweet_data, dict_corr\n",
    "\n",
    "positive_preprocessed, dict_corr1 = preprocess(positive_pd)  \n",
    "with open('Processed_data/' + 'positive_spell.txt', \"w+\", encoding = \"UTF-8\") as f: #positive_spell.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in positive_preprocessed[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)\n",
    "del positive_preprocessed\n",
    "print(\"done positive\")\n",
    "\n",
    "negative_preprocessed, dict_corr2 = preprocess(negative_pd)\n",
    "with open('Processed_data/' + 'negative_spell.txt', \"w+\", encoding = \"UTF-8\") as f: #negative_spell.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in negative_preprocessed[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)\n",
    "del negative_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### APPLYING DESIRED PREPROCESSING (to test)\n",
    "\n",
    "test_dataset = \"Datasets/test_data.txt\"\n",
    "\n",
    "def load_in_pd_test(data_path):\n",
    "    with open(data_path, encoding=\"utf-8\") as f :\n",
    "        test = f.read().splitlines()\n",
    "        tweets = []\n",
    "        ids = []\n",
    "        for line in test:\n",
    "            id, tweet = line.split(',',1)\n",
    "            tweets.append(tweet)\n",
    "            ids.append(id)\n",
    "        test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    return test\n",
    "\n",
    "test_pd = load_in_pd_test(test_dataset)\n",
    "test_spell, dict_corr_test = spell_correction(test_pd) \n",
    "\n",
    "with open(\"Processed_data/test_spell.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
    "    for index, row in test_spell.iterrows():\n",
    "        f.write(index + \",\" + row[\"Tweets\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather input for word embeddings\n",
    "with open(\"Processed_data/input_WE.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    with open(\"Processed_data/test_spell.txt\",encoding=\"utf-8\") as test:\n",
    "        test = test.readlines()\n",
    "        for line in test:\n",
    "            id, tweet = line.split(',',1)\n",
    "            f.write(\"%s\" % tweet)\n",
    "    \n",
    "    for path in ['Processed_data/positive_spell.txt','Processed_data/negative_spell.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##APPLY build_vocab.sh AND CUT THE VOCABULARY USING THE CHOSEN THRESHOLD\n",
    "\n",
    "#Choose the desired cutting parameter here (Tokens with >= cut_threshold occurrences are kept)\n",
    "cut_threshold = 5\n",
    "\n",
    "\n",
    "arg1 = 'positive_preprocessed' + time + '.txt'\n",
    "arg2 = 'negative_preprocessed' + time + '.txt'\n",
    "arg3 = 'vocab_' + time + '.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh \" + arg1 + \" \" + arg2 + \" \" + arg3)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")\n",
    "    \n",
    "def cut_and_save_vocab(file_in, file_out):\n",
    "    full_vocab = pd.read_csv(file_in, sep = \"(\\s+)\", header=None, engine = 'python')\n",
    "    cutted_vocab = full_vocab[full_vocab[0] >= cut_threshold][2]\n",
    "    with open(file_out, 'w+') as f:\n",
    "        f.write(cutted_vocab.to_string(header = False, index = False))\n",
    "    \n",
    "cut_and_save_vocab('Processed_data/vocab_' + time + '.txt', 'Processed_data/vocab_cut' + time + '.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DUMP THE BUILT VOCABULARY TO A PICKLE FILE\n",
    "vocab = dict()\n",
    "with open('Processed_data/vocab_cut' + time + '.txt') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        vocab[line.strip()] = idx\n",
    "\n",
    "with open('Processed_data/vocab_' + time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREATE A CO-OCCURRENCE MATRIX\n",
    "def create_cooc(vocab_file, negative_file, positive_file, output_file):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    vocab_size = len(vocab)\n",
    "    data, row, col = [], [], []\n",
    "    counter = 1\n",
    "    for fn in [negative_file, positive_file]:\n",
    "        with open(fn,encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "                tokens = [t for t in tokens if t >= 0]\n",
    "                for t in tokens:\n",
    "                    for t2 in tokens:\n",
    "                        data.append(1)\n",
    "                        row.append(t)\n",
    "                        col.append(t2)\n",
    "\n",
    "                if counter % 10000 == 0:\n",
    "                    print(counter)\n",
    "                counter += 1\n",
    "\n",
    "                if counter % 200000 == 0:\n",
    "                    print(len(data))\n",
    "                    cooc = coo_matrix((data, (row, col)))\n",
    "                    print(\"summing duplicates (this can take a while)\")\n",
    "                    cooc.sum_duplicates()\n",
    "                    data=list(cooc.data)\n",
    "                    row=list(cooc.row)\n",
    "                    col=list(cooc.col)\n",
    "                    print(len(data))\n",
    "\n",
    "    print(len(data))\n",
    "    cooc = coo_matrix((data, (row, col)))\n",
    "    print(\"summing duplicates (this can take a while)\")\n",
    "    cooc.sum_duplicates()\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(cooc, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "#Used datasets should be defined on the second code block\n",
    "create_cooc('Processed_data/vocab_' + time + '.pkl', 'Processed_data/negative_preprocessed' + time + '.txt', 'Processed_data/positive_preprocessed' + time + '.txt',\n",
    "            'Processed_data/cooc_pickle' + time + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPLY glove.py\n",
    "random.seed(123)\n",
    "def glove(cooc_pickle, output_file):\n",
    "    print(\"loading cooccurrence matrix\")\n",
    "    with open(cooc_pickle, 'rb') as f:\n",
    "        cooc = pickle.load(f)\n",
    "    print(\"{} nonzero entries\".format(cooc.nnz))\n",
    "\n",
    "    nmax = 100\n",
    "    print(\"using nmax =\", nmax, \", cooc.max() =\", cooc.max())\n",
    "\n",
    "    print(\"initializing embeddings\")\n",
    "    embedding_dim = 20\n",
    "    xs = np.random.normal(size=(cooc.shape[0], embedding_dim))\n",
    "    ys = np.random.normal(size=(cooc.shape[1], embedding_dim))\n",
    "\n",
    "    eta = 0.001\n",
    "    alpha = 3 / 4\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
    "            logn = np.log(n)\n",
    "            fn = min(1.0, (n / nmax) ** alpha)\n",
    "            x, y = xs[ix, :], ys[jy, :]\n",
    "            scale = 2 * eta * fn * (logn - np.dot(x, y))\n",
    "            xs[ix, :] += scale * y\n",
    "            ys[jy, :] += scale * x\n",
    "    np.save(output_file, xs)\n",
    "\n",
    "glove('Processed_data/cooc_pickle' + time + '.pkl', 'Processed_data/embeddings' + time + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PREPARE DATA FOR TRAINING A CLASSIFIER\n",
    "\n",
    "def load_train_data(data_path):\n",
    "    \"\"\"Loads data and returns ids (event ids) and X (feature vector)\"\"\"\n",
    "    f = open(data_path, encoding=\"utf-8\")\n",
    "    x = f.read().splitlines()\n",
    "    x = pd.DataFrame(x, columns=['Tweets'])\n",
    "    return x\n",
    "\n",
    "def representation(tweet, we, vocab):\n",
    "    acc = np.array(0)\n",
    "    n_ignored_word = 0\n",
    "    for word in tweet.split():\n",
    "        if word not in vocab.keys():\n",
    "            n_ignored_word += 1\n",
    "        else:\n",
    "            try:\n",
    "                acc = np.add(acc,we[vocab[word]])\n",
    "            except: \n",
    "                #print(\"problem with \" + word) #last word from vocab is missing in cooc\n",
    "                n_ignored_word += 1\n",
    "    n = len(tweet.split()) - n_ignored_word\n",
    "    acc = acc / n\n",
    "    return(acc)\n",
    "\n",
    "\n",
    "def create_train_data(positive_path, negative_path, vocab, we):\n",
    "    pos = load_train_data(positive_path)\n",
    "    neg = load_train_data(negative_path)\n",
    "    pos[\"y\"] = 1\n",
    "    neg[\"y\"] = -1\n",
    "\n",
    "    print('pos...')\n",
    "    \n",
    "    pos.reset_index(drop = True, inplace = True)\n",
    "    pos[\"w\"] = pos[\"Tweets\"].apply(lambda x: representation(x, we, vocab))\n",
    "    pos.drop(\"Tweets\", axis=1, inplace = True)\n",
    "\n",
    "    col = [\"w\" + str(k) for k in range(np.shape(we)[1])]\n",
    "    pos[col] = pos[\"w\"].apply(pd.Series)\n",
    "    pos.drop(\"w\",axis=1,inplace=True)\n",
    "    \n",
    "    #remove the tweets which do not have any words used more than 5 times in the training dataset\n",
    "    pos.dropna(inplace=True) \n",
    "    \n",
    "    \n",
    "    with open('Processed_data/pos' + time + '.pkl', 'wb') as k:\n",
    "        pickle.dump(pos, k, pickle.HIGHEST_PROTOCOL)\n",
    "    del pos\n",
    "    print('pos done')\n",
    "    \n",
    "    print('neg')\n",
    "    neg.reset_index(drop = True, inplace = True)\n",
    "    neg[\"w\"] = neg[\"Tweets\"].apply(lambda x: representation(x, we, vocab))\n",
    "    neg.drop(\"Tweets\", axis=1, inplace = True)\n",
    "\n",
    "    col = [\"w\" + str(k) for k in range(np.shape(we)[1])]\n",
    "    neg[col] = neg[\"w\"].apply(pd.Series)\n",
    "    neg.drop(\"w\",axis=1,inplace=True)\n",
    "    \n",
    "    #remove the tweets which do not have any words used more than 5 times in the training dataset\n",
    "    neg.dropna(inplace=True)\n",
    "    \n",
    "    print('neg done')\n",
    "    \n",
    "    with open('Processed_data/pos' + time + '.pkl', 'rb') as j:\n",
    "        pos = pickle.load(j)\n",
    "    \n",
    "    print('lets try to concatenate')\n",
    "    train = pd.concat([pos, neg])\n",
    "    print('tadaaaa')\n",
    "    return train\n",
    "\n",
    "with open('Processed_data/vocab_' + time + '.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "we = np.load('Processed_data/embeddings' + time + '.npy')\n",
    "#The names of the datasets are defined in the second code block\n",
    "train = create_train_data(positive_dataset, negative_dataset, vocab, we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAIN A CLASSIFIER\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "X = train.drop(\"y\", axis=1)\n",
    "y = train[\"y\"]\n",
    "random.seed(123)\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD THE TEST DATA, USE THE MODEL TO PREDICT THE SENTIMENTS AND SAVE THE PREDICTIONS\n",
    "\n",
    "def load_and_prepare_test_data(data_path,vocab, we):\n",
    "    \"\"\"Loads data and returns ids (event ids) and X (feature vector)\"\"\"\n",
    "    f = open(data_path, encoding='utf-8')\n",
    "    test = f.read().splitlines()\n",
    "    tweets = []\n",
    "    ids = []\n",
    "    for line in test:\n",
    "        id, tweet = line.split(',',1)\n",
    "        tweets.append(tweet)\n",
    "        ids.append(id)\n",
    "    test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    test[\"w\"] = test[\"Tweets\"].apply(lambda x: representation(x, we, vocab))\n",
    "    col = [\"w\" + str(k) for k in range(np.shape(we)[1])]\n",
    "    test[col] = test[\"w\"].apply(pd.Series)\n",
    "    test.drop(\"w\", axis=1, inplace = True)\n",
    "    test.drop(\"Tweets\", axis=1, inplace = True)\n",
    "    \n",
    "    drops = test[test.isnull().any(axis=1)].index\n",
    "    #for the tweets which do not have any words in the cutted vocabulary, predict 1\n",
    "    test.fillna(1, inplace=True)\n",
    "    \n",
    "    return test,drops\n",
    "\n",
    "test, drops = load_and_prepare_test_data('Datasets/test_data.txt', vocab, we)\n",
    "test[\"Prediction\"] = clf.predict(test)\n",
    "#test[\"Id\"] = test.index\n",
    "test[\"Prediction\"].to_csv(\"Submissions/submission\" + time + \".csv\", header= True, index_label = \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
