{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT USED PACKAGES AND SET SEED\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import *\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pbs\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "from IPython.display import display\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.strftime(datetime.now(), \"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove automatic tweets about pictures that occur only in the negative dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_picture_tweet(str):\n",
    "    \n",
    "\n",
    "def drop_frame():\n",
    "    with open(\"Datasets/\"+input_filename, encoding=\"utf-8\") as f :\n",
    "        tweets = f.read().splitlines()\n",
    "        df_tweets = pd.DataFrame(tweets,columns=['Tweets'])\n",
    "        df_tweets[\"Tweets\"].apply(lambda str : )\n",
    "    with open('Processed_data/no_dupl_' + input_filename, \"w+\", encoding = \"UTF-8\") as f:\n",
    "        for tweet in df_tweets[\"Tweets\"]:\n",
    "            f.write(\"%s\\n\" % tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate(input_filename):\n",
    "    with open(\"Datasets/\"+input_filename, encoding=\"utf-8\") as f :\n",
    "        tweets = f.read().splitlines()\n",
    "        df_tweets = pd.DataFrame(tweets,columns=['Tweets'])\n",
    "        df_tweets.drop_duplicates(inplace=True)\n",
    "    with open('Processed_data/no_dupl_' + input_filename, \"w+\", encoding = \"UTF-8\") as f:\n",
    "        for tweet in df_tweets[\"Tweets\"]:\n",
    "            f.write(\"%s\\n\" % tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ['train_pos.txt','train_neg.txt','train_neg_full.txt','train_pos_full.txt']:\n",
    "    drop_duplicate(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates test\n",
    "with open(\"Datasets/test_data.txt\", encoding=\"utf-8\") as f :\n",
    "    test = f.read().splitlines()\n",
    "    tweets = []\n",
    "    ids = []\n",
    "    for line in test:\n",
    "        id, tweet = line.split(',',1)\n",
    "        tweets.append(tweet)\n",
    "        ids.append(id)\n",
    "    df_test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    df_test.drop_duplicates(inplace=True)\n",
    "    \n",
    "with open('Processed_data/no_dupl_test_data.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in df_test[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate all text files (pos, neg and test) to create a dataset used as reference for twitter language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Processed_data/twitter_language.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    for path in ['Processed_data/no_dupl_train_pos.txt','Processed_data/no_dupl_train_neg.txt', 'Processed_data/no_dupl_test_data.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Processed_data/twitter_language_full.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    for path in ['Processed_data/no_dupl_train_pos_full.txt','Processed_data/no_dupl_train_neg_full.txt', 'Processed_data/no_dupl_test_data.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a frequency dictionnary of words occuring at least *threshold* times in the dataset, \n",
    "## that will be used as a reference for spell checking\n",
    "\n",
    "output_filename = 'Processed_data/vocab_freq.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/twitter_language.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a frequency dictionnary of words occuring at least *threshold* times in the dataset, \n",
    "## that will be used as a reference for spell checking\n",
    "\n",
    "output_filename = 'Processed_data/vocab_freq_full.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/twitter_language_full.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionnary(input_vocab, threshold):\n",
    "    full_vocab = pd.read_csv(input_vocab+\".txt\", sep = \"\\s+\", header=None, engine='python')\n",
    "    cut_vocab = full_vocab[full_vocab[0] >= threshold]\n",
    "    cut_vocab.columns = [\"number occ\",\"word\"]\n",
    "    cut_vocab.set_index(\"word\",inplace=True)\n",
    "    with open(input_vocab + \"_\" + str(threshold) + '.json', 'w') as f:\n",
    "        json.dump(cut_vocab[\"number occ\"].to_dict(), f)\n",
    "\n",
    "#build_dictionnary(\"Processed_data/vocab_freq\", threshold=5) #TODO try other threshold\n",
    "#build_dictionnary(\"Processed_data/vocab_freq_full\", threshold=5) #TODO try other threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spell correction functions\n",
    "def correct(tweet, dict_corr):\n",
    "    list_words = tweet.split()\n",
    "    for i, word in enumerate(list_words):\n",
    "        if word in dict_corr :\n",
    "            list_words[i] = dict_corr[word]\n",
    "    corr_tweet = ' '.join(list_words)\n",
    "    return (corr_tweet) \n",
    "\n",
    "def spell_correction(data, reference_dictionnary_filename):\n",
    "    spell = SpellChecker(distance=1) # TODO if possible, try distance=2\n",
    "    spell.word_frequency.load_dictionary(reference_dictionnary_filename) \n",
    "    dict_corr = {}\n",
    "    for tweet in data[\"Tweets\"]:\n",
    "        list_words = tweet.split()\n",
    "        for i, word in enumerate(list_words):\n",
    "            if word not in dict_corr :\n",
    "                if word in spell.unknown([word]):\n",
    "                    dict_corr[word] = spell.correction(word)\n",
    "    data[\"Tweets\"] = data[\"Tweets\"].apply(lambda x : correct(x, dict_corr))\n",
    "    return data, dict_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the used datasets here\n",
    "positive_dataset = 'Processed_data/no_dupl_train_pos_full.txt' #'Processed_data/no_dupl_train_pos_full.txt'\n",
    "negative_dataset = 'Processed_data/no_dupl_train_neg_full.txt' #'Processed_data/no_dupl_train_neg_full.txt'\n",
    "\n",
    "def load_in_pd(data_path):\n",
    "    with open(data_path, encoding=\"utf-8\") as f :\n",
    "        x = f.read().splitlines()\n",
    "        x = pd.DataFrame(x,columns=['Tweets'])\n",
    "    return (x)\n",
    "\n",
    "positive_pd = load_in_pd(positive_dataset)\n",
    "negative_pd = load_in_pd(negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### APPLYING spell correction to pos and neg\n",
    "\n",
    "positive_preprocessed, dict_corr1 = spell_correction(positive_pd,'Processed_data/vocab_freq_full.json') #'Processed_data/vocab_freq_full.json') \n",
    "with open('Processed_data/' + 'positive_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f: #positive_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in positive_preprocessed[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)\n",
    "del positive_preprocessed\n",
    "print(\"done positive\")\n",
    "\n",
    "negative_preprocessed, dict_corr2 = spell_correction(negative_pd,'Processed_data/vocab_freq_full.json') #'Processed_data/vocab_freq_full.json') \n",
    "with open('Processed_data/' + 'negative_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f: #negative_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in negative_preprocessed[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)\n",
    "del negative_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPLYING spell correction to test\n",
    "\n",
    "test_dataset = \"Datasets/test_data.txt\"\n",
    "\n",
    "def load_in_pd_test(data_path):\n",
    "    with open(data_path, encoding=\"utf-8\") as f :\n",
    "        test = f.read().splitlines()\n",
    "        tweets = []\n",
    "        ids = []\n",
    "        for line in test:\n",
    "            id, tweet = line.split(',',1)\n",
    "            tweets.append(tweet)\n",
    "            ids.append(id)\n",
    "        test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    return test\n",
    "\n",
    "test_pd = load_in_pd_test(test_dataset)\n",
    "test_spell, dict_corr_test = spell_correction(test_pd,'Processed_data/vocab_freq_full.json') #'Processed_data/vocab_freq_full.json')  \n",
    "\n",
    "with open(\"Processed_data/test_spell.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
    "    for index, row in test_spell.iterrows():\n",
    "        f.write(index + \",\" + row[\"Tweets\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one done\n",
      "one done\n",
      "one done\n",
      "one done\n",
      "Wall time: 47min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b4afb4e7b8>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "thresholds = [10,20,50,100]\n",
    "nbr_corrected = []\n",
    "for threshold in thresholds:\n",
    "    build_dictionnary(\"Processed_data/vocab_freq_full\", threshold=threshold)\n",
    "    with open(\"Processed_data/twitter_language_full.txt\", encoding=\"utf-8\") as f :\n",
    "        x = f.read().splitlines()\n",
    "        x = pd.DataFrame(x,columns=['Tweets'])\n",
    "    output, dict_corr = spell_correction(x,\"Processed_data/vocab_freq_full_\" + str(threshold) + '.json')\n",
    "    corrected = {k:v for (k,v) in dict_corr.items() if v==v}\n",
    "    nbr_corrected.append(len(corrected))\n",
    "    print(\"one done\")\n",
    "plt.plot(thresholds,nbr_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "thresholds = [10,20,50,100]\n",
    "dictionaries = []\n",
    "nbr_corrected = []\n",
    "ratio_corrected = []\n",
    "for threshold in thresholds:\n",
    "    build_dictionnary(\"Processed_data/vocab_freq_full\", threshold=threshold)\n",
    "    with open(\"Processed_data/twitter_language_full.txt\", encoding=\"utf-8\") as f :\n",
    "        x = f.read().splitlines()\n",
    "        x = pd.DataFrame(x,columns=['Tweets'])\n",
    "    output, dict_corr = spell_correction(x,\"Processed_data/vocab_freq_full_\" + str(threshold) + '.json')\n",
    "    dictionaries.append(dict_corr)\n",
    "    corrected = {k:v for (k,v) in dict_corr.items() if v==v}\n",
    "    nbr_corrected.append(len(corrected))\n",
    "    ratio_corrected.append(len(corrected)/len(dict_corr))\n",
    "    print(\"one done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds,nbr_corrected)\n",
    "plt.plot(thresholds,ratio_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather input for word embeddings\n",
    "with open(\"Processed_data/input_WE_full.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    with open(\"Processed_data/test_spell.txt\",encoding=\"utf-8\") as test:\n",
    "        test = test.readlines()\n",
    "        for line in test:\n",
    "            id, tweet = line.split(',',1)\n",
    "            f.write(\"%s\" % tweet)\n",
    "    \n",
    "    for path in ['Processed_data/positive_spell_full.txt','Processed_data/negative_spell_full.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'Processed_data/vocab_freq_after_spell_full.txt' #'Processed_data/vocab_freq_after_spell_full.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/input_WE_full.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find good minimal threshold on the number of occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab_spell = pd.read_csv('Processed_data/vocab_freq_after_spell_full.txt', sep = \"\\s+\", header=None, engine='python')\n",
    "full_vocab_spell.columns = [\"number occ\",\"word\"]\n",
    "full_vocab_spell.set_index(\"word\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,7))\n",
    "plt.yscale(\"log\")\n",
    "plt.hist(full_vocab_spell[\"number occ\"],bins=200)\n",
    "plt.xlabel(\"Number of occurence\")\n",
    "plt.ylabel(\"Number of word\")\n",
    "plt.title(\"Histogram of the number of occurences of all different words in the whole dataset\")\n",
    "plt.fig_save(\"Plot/threshold_build_vocab_fig1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_vocab_spell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5a2c5364b15a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_vocab_spell\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"number occ\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of occurence\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of word\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_vocab_spell' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize=(15,7))\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.hist(full_vocab_spell[\"number occ\"],bins=2**(np.arange(0,24)))\n",
    "plt.xlabel(\"Number of occurence\")\n",
    "plt.ylabel(\"Number of word\")\n",
    "plt.title(\"Histogram of the number of occurences of all different words in the whole dataset\")\n",
    "plt.fig_save(\"Plot/threshold_build_vocab_fig2.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a power-law distribution !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,12))\n",
    "x = range(1,100)\n",
    "y1 = []\n",
    "y2 = []\n",
    "for threshold in x :\n",
    "    y1.append(sum(full_vocab_spell[\"number occ\"]>threshold))\n",
    "    y2.append(sum(full_vocab_spell[full_vocab_spell[\"number occ\"]>threshold][\"number occ\"]))\n",
    "plt.subplot(211)\n",
    "plt.xlabel(\"Minimal threshold of occurences\")\n",
    "plt.ylabel(\"Number of different word\")\n",
    "plt.ylim(0,max(y1)*1.1)\n",
    "plt.plot(x,y1)\n",
    "plt.subplot(212)\n",
    "plt.xlabel(\"Minimal threshold of occurences\")\n",
    "plt.ylabel(\"Total number of words\")\n",
    "plt.ylim(0,max(y2)*1.1)\n",
    "plt.plot(x,y2)\n",
    "plt.fig_save(\"Plot/threshold_build_vocab_fig3.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"log\")\n",
    "plt.hist(full_vocab_spell[\"number occ\"],bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose threshold = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab_spell[full_vocab_spell[\"number occ\"]>20000].sort_values(by=\"number occ\").to_csv(\"Stops\" + time + \".csv\", header= True, index_label = \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##APPLY build_vocab.sh AND CUT THE VOCABULARY USING THE CHOSEN THRESHOLD\n",
    "\n",
    "#Choose the desired cutting parameter here (Tokens with >= cut_threshold occurrences are kept)\n",
    "cut_threshold = 5\n",
    "\n",
    "output_filename = 'Processed_data/vocab_full' + time + '.txt'\n",
    "vocab_successful = os.system(\"sh build_vocab.sh Processed_data/input_WE_full.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")\n",
    "    \n",
    "def cut_and_save_vocab(file_in, file_out):\n",
    "    full_vocab = pd.read_csv(file_in, sep = \"\\s+\", header=None, engine = 'python')\n",
    "    cutted_vocab = full_vocab[full_vocab[0] >= cut_threshold][2]\n",
    "    with open(file_out, 'w+') as f:\n",
    "        f.write(cutted_vocab.to_string(header = False, index = False))\n",
    "    \n",
    "cut_and_save_vocab('Processed_data/vocab_full' + time + '.txt', 'Processed_data/vocab_cut_full' + time + '.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DUMP THE BUILT VOCABULARY TO A PICKLE FILE\n",
    "vocab = dict()\n",
    "with open('Processed_data/vocab_cut_full' + time + '.txt') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        vocab[line.strip()] = idx\n",
    "\n",
    "with open('Processed_data/vocab_full' + time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##CREATE A CO-OCCURRENCE MATRIX with context defined with threshold distance\n",
    "def context(lst,index,threshold):\n",
    "    return(lst[index+1 : min(index+threshold, len(lst)-1)])\n",
    "\n",
    "def create_cooc_dist(vocab_file, negative_file, positive_file, output_file, threshold):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    vocab_size = len(vocab)\n",
    "    data, row, col = [], [], []\n",
    "    counter = 1\n",
    "    for fn in [negative_file, positive_file]:\n",
    "        with open(fn,encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "                #tokens = [t for t in tokens if t >= 0]\n",
    "                for i, t1 in enumerate(tokens):\n",
    "                    if t1 >= 0:\n",
    "                        for t2 in context(tokens,i,threshold):\n",
    "                            if t2 >= 0:\n",
    "                                if t1==t2 :\n",
    "                                    data.append(1/2)\n",
    "                                else :\n",
    "                                    data.append(1)\n",
    "                                row.append(min(t1,t2))\n",
    "                                col.append(max(t1,t2))\n",
    "\n",
    "                if counter % 100000 == 0:\n",
    "                    print(counter)\n",
    "                counter += 1\n",
    "\n",
    "                if counter % 3000000 == 0:\n",
    "                    print(len(data))\n",
    "                    cooc = coo_matrix((data, (row, col)))\n",
    "                    print(\"summing duplicates (this can take a while)\")\n",
    "                    cooc.sum_duplicates()\n",
    "                    data=list(cooc.data)\n",
    "                    row=list(cooc.row)\n",
    "                    col=list(cooc.col)\n",
    "                    print(len(data))\n",
    "\n",
    "    print(len(data))\n",
    "    final_row = row+col\n",
    "    final_col = col+row\n",
    "    data = data+data\n",
    "    cooc = coo_matrix((data, (final_row, final_col)))\n",
    "    print(\"Summing duplicates\")\n",
    "    cooc.sum_duplicates()\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(cooc, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "#Used datasets should be defined on the second code block\n",
    "create_cooc_dist('Processed_data/vocab_full' + time + '.pkl', 'Processed_data/negative_spell_full.txt', 'Processed_data/positive_spell_full.txt',\n",
    "            'Processed_data/cooc_dist_pickle_full' + time + '.pkl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not create word embeddings for these stop words\n",
    "def drop_stop_rows(cooc_path, stop_path, vocab_path, output_path):\n",
    "    with open(cooc_path, 'rb') as f:\n",
    "        cooc = pickle.load(f)\n",
    "    with open(stop_path, encoding = \"utf-8\") as stoplist:\n",
    "        words = stoplist.read().splitlines()\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    stop_indexes = [vocab.get(t, -1) for t in words]\n",
    "    keeps = np.logical_not(np.isin(cooc.row, stop_indexes))\n",
    "    print(np.mean(keeps))\n",
    "    new_cols = cooc.col[keeps]\n",
    "    new_rows = cooc.row[keeps]\n",
    "    new_data = cooc.data[keeps]\n",
    "    new_cooc = coo_matrix((new_data, (new_rows, new_cols)))\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(new_cooc, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "drop_stop_rows('Processed_data/cooc_dist_pickle_full' + time + '.pkl', \"Stoplist.txt\", \n",
    "              'Processed_data/vocab_full' + time + '.pkl', 'Processed_data/cooc_dist_pickle_cutted' + time + ''.pkl')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE WORD EMBEDDINGS\n",
    "random.seed(123)\n",
    "def glove(cooc_pickle, output_file):\n",
    "    print(\"loading cooccurrence matrix\")\n",
    "    with open(cooc_pickle, 'rb') as f:\n",
    "        cooc = pickle.load(f)\n",
    "    print(\"{} nonzero entries\".format(cooc.nnz))\n",
    "\n",
    "    nmax = 100\n",
    "    print(\"using nmax =\", nmax, \", cooc.max() =\", cooc.max())\n",
    "\n",
    "    print(\"initializing embeddings\")\n",
    "    embedding_dim = 20\n",
    "    xs = np.random.normal(size=(cooc.shape[0], embedding_dim))\n",
    "    ys = np.random.normal(size=(cooc.shape[1], embedding_dim))\n",
    "\n",
    "    eta = 0.001\n",
    "    alpha = 3 / 4\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
    "            logn = np.log(n)\n",
    "            fn = min(1.0, (n / nmax) ** alpha)\n",
    "            x, y = xs[ix, :], ys[jy, :]\n",
    "            scale = 2 * eta * fn * (logn - np.dot(x, y))\n",
    "            xs[ix, :] += scale * y\n",
    "            ys[jy, :] += scale * x\n",
    "    np.save(output_file, xs)\n",
    "\n",
    "glove('Processed_data/cooc_dist_pickle_cutted' + time + \".pkl\", 'Processed_data/embeddings' + time + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD THE TEST DATA, USE THE MODEL TO PREDICT THE SENTIMENTS AND SAVE THE PREDICTIONS\n",
    "with open('Processed_data/vocab_full' + time + '.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "f.close()\n",
    "we = np.load('Processed_data/embeddings' + time + '.npy')\n",
    "\n",
    "#Define the used datasets here\n",
    "positive_path = 'Processed_data/positive_spell_full.txt'\n",
    "negative_path = 'Processed_data/negative_spell_full.txt'\n",
    "\n",
    "def load_train_data(data_path):\n",
    "    \"\"\"Loads data and returns ids (event ids) and X (feature vector)\"\"\"\n",
    "    f = open(data_path, encoding=\"utf-8\")\n",
    "    x = f.readlines()\n",
    "    x = pd.DataFrame(x, columns=['Tweets'])\n",
    "    f.close()\n",
    "    return x\n",
    "\n",
    "def load_and_label(negative_path, positive_path):\n",
    "    pos = load_train_data(positive_path)\n",
    "    neg = load_train_data(negative_path)\n",
    "    pos[\"y\"] = 1\n",
    "    neg[\"y\"] = 0\n",
    "    train = pd.concat([pos, neg])\n",
    "    train.reset_index(drop = True, inplace = True)\n",
    "    return train\n",
    "\n",
    "\n",
    "train = load_and_label(negative_path, positive_path)\n",
    "\n",
    "def count_tokens(line, vocab):\n",
    "    tokens = line.split()\n",
    "    nof_tokens = 0\n",
    "    for token in tokens:\n",
    "        if token in vocab.keys():\n",
    "            nof_tokens += 1\n",
    "    return nof_tokens\n",
    "\n",
    "train[\"nof_tokens\"] = train[\"Tweets\"].apply(lambda x: count_tokens(x, vocab))\n",
    "\n",
    "max_tokens = max(train[\"nof_tokens\"])\n",
    "max_tokens = 25\n",
    "train = train[train[\"nof_tokens\"] <= max_tokens]\n",
    "train.reset_index(drop = True, inplace = True)\n",
    "\n",
    "def data_to_matrix_array(train, max_tokens, we, vocab, stop_path):\n",
    "    with open(stop_path, encoding = \"utf-8\") as stoplist:\n",
    "        stop_words = stoplist.read().splitlines()\n",
    "    train_data = np.zeros((len(train), max_tokens, np.shape(we)[1]), dtype = np.float32)\n",
    "    i = -1\n",
    "    stop_indexes = [vocab.get(t, -1) for t in stop_words]\n",
    "    valid_keys = list(filter(lambda x: x not in stop_indexes, vocab.keys()))\n",
    "    filtered_vocab = {key: vocab[key] for key in valid_keys}\n",
    "    for line in train[\"Tweets\"]:\n",
    "        i += 1\n",
    "        if i % 20000 == 0:\n",
    "            print(i)\n",
    "        count = max_tokens\n",
    "        tokens = line.split()\n",
    "        for t in tokens:\n",
    "            if t in filtered_vocab.keys():\n",
    "                count -= 1\n",
    "                train_data[i, count, :] = we[filtered_vocab[t]]\n",
    "    return train_data\n",
    "    \n",
    "\n",
    "train_data = data_to_matrix_array(train, max_tokens, we, vocab, 'Stoplist.txt')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS NEEDED FOR BUILDING A CLASSIFIER\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_tokens, dim):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(80, dropout = 0.2, input_shape=(max_tokens, dim), recurrent_dropout = 0.2))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    #model.add(LSTM(60, input_shape=(max_tokens, dim), return_sequences=True, name='input_layer'))\n",
    "    #model.add(Dropout(0.15))\n",
    "    #model.add(Flatten())\n",
    "\n",
    "    #model.add(Dense(5, activation='relu', name='dense1'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    #model.add(Dense(5, activation='relu', name='dense2'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    #model.add(Dense(5, activation='sigmoid', name='dense3'))\n",
    "    #model.add(Dropout(0.1))\n",
    "\n",
    "    #model.add(Dense(1, activation='sigmoid', name='Out_layer'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss = 'binary_crossentropy',\n",
    "        optimizer = 'adam',\n",
    "        metrics = ['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "subset = np.random.uniform(0, 1, len(train_data)) < 1.1 #Percentage of data to be used to fit the nn\n",
    "y_sample = train[\"y\"][subset]\n",
    "y_sample.reset_index(drop = True, inplace = True)\n",
    "classifier = create_model(max_tokens, np.shape(we)[1])\n",
    "fit = classifier.fit(x = train_data[subset, :, :], y = y_sample, batch_size=4, epochs=2, verbose=1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD THE TEST DATA, USE THE MODEL TO PREDICT THE SENTIMENTS AND SAVE THE PREDICTIONS\n",
    "\n",
    "def load_and_prepare_test_data(data_path,vocab, we):\n",
    "    \"\"\"Loads data and returns ids (event ids) and X (feature vector)\"\"\"\n",
    "    f = open(data_path, encoding='utf-8')\n",
    "    test = f.read().splitlines()\n",
    "    tweets = []\n",
    "    ids = []\n",
    "    for line in test:\n",
    "        id, tweet = line.split(',',1)\n",
    "        tweets.append(tweet)\n",
    "        ids.append(id)\n",
    "    test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    test[\"w\"] = test[\"Tweets\"].apply(lambda x: representation(x, we, vocab))\n",
    "    col = [\"w\" + str(k) for k in range(np.shape(we)[1])]\n",
    "    test[col] = test[\"w\"].apply(pd.Series)\n",
    "    test.drop(\"w\", axis=1, inplace = True)\n",
    "    test.drop(\"Tweets\", axis=1, inplace = True)\n",
    "    \n",
    "    drops = test[test.isnull().any(axis=1)].index\n",
    "    #for the tweets which do not have any words in the cutted vocabulary, predict 1\n",
    "    test.fillna(1, inplace=True)\n",
    "    \n",
    "    return test,drops\n",
    "\n",
    "test, drops = load_and_prepare_test_data('Datasets/test_data.txt', vocab, we)\n",
    "test[\"Prediction\"] = clf.predict(test)\n",
    "#test[\"Id\"] = test.index\n",
    "test[\"Prediction\"].to_csv(\"Submissions/submission\" + time + \".csv\", header= True, index_label = \"Id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
