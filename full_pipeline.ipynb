{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT USED PACKAGES AND SET SEED\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import *\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pbs\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "from IPython.display import display\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.strftime(datetime.now(), \"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate(input_filename):\n",
    "    with open(\"Datasets/\"+input_filename, encoding=\"utf-8\") as f :\n",
    "        tweets = f.read().splitlines()\n",
    "        df_tweets = pd.DataFrame(tweets,columns=['Tweets'])\n",
    "        df_tweets.drop_duplicates(inplace=True)\n",
    "    with open('Processed_data/no_dupl_' + input_filename, \"w+\", encoding = \"UTF-8\") as f:\n",
    "        for tweet in df_tweets[\"Tweets\"]:\n",
    "            f.write(\"%s\\n\" % tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ['train_pos.txt','train_neg.txt','train_neg_full.txt','train_pos_full.txt']:\n",
    "    drop_duplicate(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates test\n",
    "with open(\"Datasets/test_data.txt\", encoding=\"utf-8\") as f :\n",
    "    test = f.read().splitlines()\n",
    "    tweets = []\n",
    "    ids = []\n",
    "    for line in test:\n",
    "        id, tweet = line.split(',',1)\n",
    "        tweets.append(tweet)\n",
    "        ids.append(id)\n",
    "    df_test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    df_test.drop_duplicates(inplace=True)\n",
    "    \n",
    "with open('Processed_data/no_dupl_test_data.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in df_test[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate all text files (pos, neg and test) to create a dataset used as reference for twitter language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Processed_data/twitter_language.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    for path in ['Processed_data/no_dupl_train_pos.txt','Processed_data/no_dupl_train_neg.txt', 'Processed_data/no_dupl_test_data.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Processed_data/twitter_language_full.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    for path in ['Processed_data/no_dupl_train_pos_full.txt','Processed_data/no_dupl_train_neg_full.txt', 'Processed_data/no_dupl_test_data.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a frequency dictionnary of words occuring at least *threshold* times in the dataset, \n",
    "## that will be used as a reference for spell checking\n",
    "\n",
    "output_filename = 'Processed_data/vocab_freq.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/twitter_language.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a frequency dictionnary of words occuring at least *threshold* times in the dataset, \n",
    "## that will be used as a reference for spell checking\n",
    "\n",
    "output_filename = 'Processed_data/vocab_freq_full.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/twitter_language_full.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionnary(input_vocab, threshold):\n",
    "    full_vocab = pd.read_csv(input_vocab+\".txt\", sep = \"\\s+\", header=None, engine='python')\n",
    "    cut_vocab = full_vocab[full_vocab[0] >= threshold]\n",
    "    cut_vocab.columns = [\"number occ\",\"word\"]\n",
    "    cut_vocab.set_index(\"word\",inplace=True)\n",
    "    with open(input_vocab + \"_\" + str(threshold) + '.json', 'w') as f:\n",
    "        json.dump(cut_vocab[\"number occ\"].to_dict(), f)\n",
    "\n",
    "#build_dictionnary(\"Processed_data/vocab_freq\", threshold=5) #TODO try other threshold\n",
    "#build_dictionnary(\"Processed_data/vocab_freq_full\", threshold=5) #TODO try other threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spell correction functions\n",
    "def correct(tweet, dict_corr):\n",
    "    list_words = tweet.split()\n",
    "    for i, word in enumerate(list_words):\n",
    "        if word in dict_corr :\n",
    "            list_words[i] = dict_corr[word]\n",
    "    corr_tweet = ' '.join(list_words)\n",
    "    return (corr_tweet) \n",
    "\n",
    "def spell_correction(data, reference_dictionnary_filename):\n",
    "    spell = SpellChecker(distance=1) # TODO if possible, try distance=2\n",
    "    spell.word_frequency.load_dictionary(reference_dictionnary_filename) \n",
    "    dict_corr = {}\n",
    "    for tweet in data[\"Tweets\"]:\n",
    "        list_words = tweet.split()\n",
    "        for i, word in enumerate(list_words):\n",
    "            if word not in dict_corr :\n",
    "                if word in spell.unknown([word]):\n",
    "                    dict_corr[word] = spell.correction(word)\n",
    "    data[\"Tweets\"] = data[\"Tweets\"].apply(lambda x : correct(x, dict_corr))\n",
    "    return data, dict_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the used datasets here\n",
    "positive_dataset = 'Processed_data/no_dupl_train_pos_full.txt' #'Processed_data/no_dupl_train_pos_full.txt'\n",
    "negative_dataset = 'Processed_data/no_dupl_train_neg_full.txt' #'Processed_data/no_dupl_train_neg_full.txt'\n",
    "\n",
    "def load_in_pd(data_path):\n",
    "    with open(data_path, encoding=\"utf-8\") as f :\n",
    "        x = f.read().splitlines()\n",
    "        x = pd.DataFrame(x,columns=['Tweets'])\n",
    "    return (x)\n",
    "\n",
    "positive_pd = load_in_pd(positive_dataset)\n",
    "negative_pd = load_in_pd(negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### APPLYING spell correction to pos and neg\n",
    "\n",
    "positive_preprocessed, dict_corr1 = spell_correction(positive_pd,'Processed_data/vocab_freq_full.json') #'Processed_data/vocab_freq_full.json') \n",
    "with open('Processed_data/' + 'positive_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f: #positive_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in positive_preprocessed[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)\n",
    "del positive_preprocessed\n",
    "print(\"done positive\")\n",
    "\n",
    "negative_preprocessed, dict_corr2 = spell_correction(negative_pd,'Processed_data/vocab_freq_full.json') #'Processed_data/vocab_freq_full.json') \n",
    "with open('Processed_data/' + 'negative_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f: #negative_spell_full.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in negative_preprocessed[\"Tweets\"]:\n",
    "        f.write(\"%s\\n\" % tweet)\n",
    "del negative_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPLYING spell correction to test\n",
    "\n",
    "test_dataset = \"Datasets/test_data.txt\"\n",
    "\n",
    "def load_in_pd_test(data_path):\n",
    "    with open(data_path, encoding=\"utf-8\") as f :\n",
    "        test = f.read().splitlines()\n",
    "        tweets = []\n",
    "        ids = []\n",
    "        for line in test:\n",
    "            id, tweet = line.split(',',1)\n",
    "            tweets.append(tweet)\n",
    "            ids.append(id)\n",
    "        test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    return test\n",
    "\n",
    "test_pd = load_in_pd_test(test_dataset)\n",
    "test_spell, dict_corr_test = spell_correction(test_pd,'Processed_data/vocab_freq_full.json') #'Processed_data/vocab_freq_full.json')  \n",
    "\n",
    "with open(\"Processed_data/test_spell.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
    "    for index, row in test_spell.iterrows():\n",
    "        f.write(index + \",\" + row[\"Tweets\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "thresholds = [10,20,50,100]\n",
    "nbr_corrected = []\n",
    "for threshold in thresholds:\n",
    "    build_dictionnary(\"Processed_data/vocab_freq_full\", threshold=threshold)\n",
    "    with open(\"Processed_data/twitter_language_full.txt\", encoding=\"utf-8\") as f :\n",
    "        x = f.read().splitlines()\n",
    "        x = pd.DataFrame(x,columns=['Tweets'])\n",
    "    output, dict_corr = spell_correction(x,\"Processed_data/vocab_freq_full_\" + str(threshold) + '.json')\n",
    "    corrected = {k:v for (k,v) in dict_corr.items() if v==v}\n",
    "    nbr_corrected.append(len(corrected))\n",
    "    print(\"one done\")\n",
    "plt.plot(thresholds,nbr_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather input for word embeddings\n",
    "with open(\"Processed_data/input_WE_full.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    with open(\"Processed_data/test_spell.txt\",encoding=\"utf-8\") as test:\n",
    "        test = test.readlines()\n",
    "        for line in test:\n",
    "            id, tweet = line.split(',',1)\n",
    "            f.write(\"%s\" % tweet)\n",
    "    \n",
    "    for path in ['Processed_data/positive_spell_full.txt','Processed_data/negative_spell_full.txt']:\n",
    "        with open(path,encoding=\"utf-8\") as file:\n",
    "            f.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'Processed_data/vocab_freq_after_spell_full.txt' #'Processed_data/vocab_freq_after_spell_full.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh Processed_data/input_WE_full.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find good minimal threshold on the number of occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab_spell = pd.read_csv('Processed_data/vocab_freq_after_spell_full.txt', sep = \"\\s+\", header=None, engine='python')\n",
    "full_vocab_spell.columns = [\"number occ\",\"word\"]\n",
    "full_vocab_spell.set_index(\"word\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,7))\n",
    "plt.yscale(\"log\")\n",
    "plt.hist(full_vocab_spell[\"number occ\"],bins=200)\n",
    "plt.xlabel(\"Number of occurence\")\n",
    "plt.ylabel(\"Number of word\")\n",
    "plt.title(\"Histogram of the number of occurences of all different words in the whole dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,7))\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.hist(full_vocab_spell[\"number occ\"],bins=2**(np.arange(0,24)))\n",
    "plt.xlabel(\"Number of occurence\")\n",
    "plt.ylabel(\"Number of word\")\n",
    "plt.title(\"Histogram of the number of occurences of all different words in the whole dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a power-law distribution !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,12))\n",
    "x = range(1,100)\n",
    "y1 = []\n",
    "y2 = []\n",
    "for threshold in x :\n",
    "    y1.append(sum(full_vocab_spell[\"number occ\"]>threshold))\n",
    "    y2.append(sum(full_vocab_spell[full_vocab_spell[\"number occ\"]>threshold][\"number occ\"]))\n",
    "plt.subplot(211)\n",
    "plt.xlabel(\"Minimal threshold of occurences\")\n",
    "plt.ylabel(\"Number of different word\")\n",
    "plt.title(\"\")\n",
    "plt.ylim(0,max(y1)*1.1)\n",
    "plt.plot(x,y1)\n",
    "plt.subplot(212)\n",
    "plt.xlabel(\"Minimal threshold of occurences\")\n",
    "plt.ylabel(\"Total number of words\")\n",
    "plt.title(\"\")\n",
    "plt.ylim(0,max(y2)*1.1)\n",
    "plt.plot(x,y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"log\")\n",
    "plt.hist(full_vocab_spell[\"number occ\"],bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose threshold = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab_spell[full_vocab_spell[\"number occ\"]>20000].sort_values(by=\"number occ\").to_csv(\"Stops\" + time + \".csv\", header= True, index_label = \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##APPLY build_vocab.sh AND CUT THE VOCABULARY USING THE CHOSEN THRESHOLD\n",
    "\n",
    "#Choose the desired cutting parameter here (Tokens with >= cut_threshold occurrences are kept)\n",
    "cut_threshold = 5\n",
    "\n",
    "output_filename = 'Processed_data/vocab_full' + time + '.txt'\n",
    "vocab_successful = os.system(\"sh build_vocab.sh Processed_data/input_WE_full.txt \" + output_filename)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")\n",
    "    \n",
    "def cut_and_save_vocab(file_in, file_out):\n",
    "    full_vocab = pd.read_csv(file_in, sep = \"\\s+\", header=None, engine = 'python')\n",
    "    cutted_vocab = full_vocab[full_vocab[0] >= cut_threshold][2]\n",
    "    with open(file_out, 'w+') as f:\n",
    "        f.write(cutted_vocab.to_string(header = False, index = False))\n",
    "    \n",
    "cut_and_save_vocab('Processed_data/vocab_full' + time + '.txt', 'Processed_data/vocab_cut_full' + time + '.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DUMP THE BUILT VOCABULARY TO A PICKLE FILE\n",
    "vocab = dict()\n",
    "with open('Processed_data/vocab_cut_full' + time + '.txt') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        vocab[line.strip()] = idx\n",
    "\n",
    "with open('Processed_data/vocab_full' + time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##CREATE A CO-OCCURRENCE MATRIX with context defined with threshold distance\n",
    "def context(lst,index,threshold):\n",
    "    return(lst[index+1 : min(index+threshold, len(lst)-1)])\n",
    "\n",
    "def create_cooc_dist(vocab_file, negative_file, positive_file, output_file, threshold):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    vocab_size = len(vocab)\n",
    "    data, row, col = [], [], []\n",
    "    counter = 1\n",
    "    for fn in [negative_file, positive_file]:\n",
    "        with open(fn,encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "                #tokens = [t for t in tokens if t >= 0]\n",
    "                for i, t1 in enumerate(tokens):\n",
    "                    if t1 >= 0:\n",
    "                        for t2 in context(tokens,i,threshold):\n",
    "                            if t2 >= 0:\n",
    "                                if t1==t2 :\n",
    "                                    data.append(1/2)\n",
    "                                else :\n",
    "                                    data.append(1)\n",
    "                                row.append(min(t1,t2))\n",
    "                                col.append(max(t1,t2))\n",
    "\n",
    "                if counter % 100000 == 0:\n",
    "                    print(counter)\n",
    "                counter += 1\n",
    "\n",
    "                if counter % 3000000 == 0:\n",
    "                    print(len(data))\n",
    "                    cooc = coo_matrix((data, (row, col)))\n",
    "                    print(\"summing duplicates (this can take a while)\")\n",
    "                    cooc.sum_duplicates()\n",
    "                    data=list(cooc.data)\n",
    "                    row=list(cooc.row)\n",
    "                    col=list(cooc.col)\n",
    "                    print(len(data))\n",
    "\n",
    "    print(len(data))\n",
    "    final_row = row+col\n",
    "    final_col = col+row\n",
    "    data = data+data\n",
    "    cooc = coo_matrix((data, (final_row, final_col)))\n",
    "    print(\"Summing duplicates\")\n",
    "    cooc.sum_duplicates()\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(cooc, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "#Used datasets should be defined on the second code block\n",
    "create_cooc_dist('Processed_data/vocab_full' + time + '.pkl', 'Processed_data/negative_spell_full.txt', 'Processed_data/positive_spell_full.txt',\n",
    "            'Processed_data/cooc_dist_pickle_full' + time + '.pkl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not create word embeddings for these stop words\n",
    "def drop_stop_rows(cooc_path, stop_path, vocab_path, output_path):\n",
    "    with open(cooc_path, 'rb') as f:\n",
    "        cooc = pickle.load(f)\n",
    "    with open(stop_path, encoding = \"utf-8\") as stoplist:\n",
    "        words = stoplist.read().splitlines()\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    stop_indexes = [vocab.get(t, -1) for t in words]\n",
    "    keeps = np.logical_not(np.isin(cooc.row, stop_indexes))\n",
    "    print(np.mean(keeps))\n",
    "    new_cols = cooc.col[keeps]\n",
    "    new_rows = cooc.row[keeps]\n",
    "    new_data = cooc.data[keeps]\n",
    "    new_cooc = coo_matrix((new_data, (new_rows, new_cols)))\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(new_cooc, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "drop_stop_rows('Processed_data/cooc_dist_pickle_full' + time + '.pkl', \"Stoplist.txt\", \n",
    "              'Processed_data/vocab_full' + time + '.pkl', 'Processed_data/cooc_dist_pickle_cutted' + time + ''.pkl')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE WORD EMBEDDINGS\n",
    "random.seed(123)\n",
    "def glove(cooc_pickle, output_file):\n",
    "    print(\"loading cooccurrence matrix\")\n",
    "    with open(cooc_pickle, 'rb') as f:\n",
    "        cooc = pickle.load(f)\n",
    "    print(\"{} nonzero entries\".format(cooc.nnz))\n",
    "\n",
    "    nmax = 100\n",
    "    print(\"using nmax =\", nmax, \", cooc.max() =\", cooc.max())\n",
    "\n",
    "    print(\"initializing embeddings\")\n",
    "    embedding_dim = 20\n",
    "    xs = np.random.normal(size=(cooc.shape[0], embedding_dim))\n",
    "    ys = np.random.normal(size=(cooc.shape[1], embedding_dim))\n",
    "\n",
    "    eta = 0.001\n",
    "    alpha = 3 / 4\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
    "            logn = np.log(n)\n",
    "            fn = min(1.0, (n / nmax) ** alpha)\n",
    "            x, y = xs[ix, :], ys[jy, :]\n",
    "            scale = 2 * eta * fn * (logn - np.dot(x, y))\n",
    "            xs[ix, :] += scale * y\n",
    "            ys[jy, :] += scale * x\n",
    "    np.save(output_file, xs)\n",
    "\n",
    "glove('Processed_data/cooc_dist_pickle_cutted' + time + \".pkl\", 'Processed_data/embeddings' + time + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD THE TEST DATA, USE THE MODEL TO PREDICT THE SENTIMENTS AND SAVE THE PREDICTIONS\n",
    "with open('Processed_data/vocab_full' + time + '.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "f.close()\n",
    "we = np.load('Processed_data/embeddings' + time + '.npy')\n",
    "\n",
    "#Define the used datasets here\n",
    "positive_path = 'Processed_data/positive_spell_full.txt'\n",
    "negative_path = 'Processed_data/negative_spell_full.txt'\n",
    "\n",
    "def load_train_data(data_path):\n",
    "    \"\"\"Loads data and returns ids (event ids) and X (feature vector)\"\"\"\n",
    "    f = open(data_path, encoding=\"utf-8\")\n",
    "    x = f.readlines()\n",
    "    x = pd.DataFrame(x, columns=['Tweets'])\n",
    "    f.close()\n",
    "    return x\n",
    "\n",
    "def load_and_label(negative_path, positive_path):\n",
    "    pos = load_train_data(positive_path)\n",
    "    neg = load_train_data(negative_path)\n",
    "    pos[\"y\"] = 1\n",
    "    neg[\"y\"] = 0\n",
    "    train = pd.concat([pos, neg])\n",
    "    train.reset_index(drop = True, inplace = True)\n",
    "    return train\n",
    "\n",
    "\n",
    "train = load_and_label(negative_path, positive_path)\n",
    "\n",
    "def count_tokens(line, vocab):\n",
    "    tokens = line.split()\n",
    "    nof_tokens = 0\n",
    "    for token in tokens:\n",
    "        if token in vocab.keys():\n",
    "            nof_tokens += 1\n",
    "    return nof_tokens\n",
    "\n",
    "train[\"nof_tokens\"] = train[\"Tweets\"].apply(lambda x: count_tokens(x, vocab))\n",
    "\n",
    "max_tokens = max(train[\"nof_tokens\"])\n",
    "max_tokens = 25\n",
    "train = train[train[\"nof_tokens\"] <= max_tokens]\n",
    "train.reset_index(drop = True, inplace = True)\n",
    "\n",
    "def data_to_matrix_array(train, max_tokens, we, vocab, stop_path):\n",
    "    with open(stop_path, encoding = \"utf-8\") as stoplist:\n",
    "        stop_words = stoplist.read().splitlines()\n",
    "    train_data = np.zeros((len(train), max_tokens, np.shape(we)[1]), dtype = np.float32)\n",
    "    i = -1\n",
    "    stop_indexes = [vocab.get(t, -1) for t in stop_words]\n",
    "    valid_keys = list(filter(lambda x: x not in stop_indexes, vocab.keys()))\n",
    "    filtered_vocab = {key: vocab[key] for key in valid_keys}\n",
    "    for line in train[\"Tweets\"]:\n",
    "        i += 1\n",
    "        if i % 20000 == 0:\n",
    "            print(i)\n",
    "        count = max_tokens\n",
    "        tokens = line.split()\n",
    "        for t in tokens:\n",
    "            if t in filtered_vocab.keys():\n",
    "                count -= 1\n",
    "                train_data[i, count, :] = we[filtered_vocab[t]]\n",
    "    return train_data\n",
    "    \n",
    "\n",
    "train_data = data_to_matrix_array(train, max_tokens, we, vocab, 'Stoplist.txt')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS NEEDED FOR BUILDING A CLASSIFIER\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_tokens, dim):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(80, dropout = 0.2, input_shape=(max_tokens, dim), recurrent_dropout = 0.2))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    #model.add(LSTM(60, input_shape=(max_tokens, dim), return_sequences=True, name='input_layer'))\n",
    "    #model.add(Dropout(0.15))\n",
    "    #model.add(Flatten())\n",
    "\n",
    "    #model.add(Dense(5, activation='relu', name='dense1'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    #model.add(Dense(5, activation='relu', name='dense2'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    #model.add(Dense(5, activation='sigmoid', name='dense3'))\n",
    "    #model.add(Dropout(0.1))\n",
    "\n",
    "    #model.add(Dense(1, activation='sigmoid', name='Out_layer'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss = 'binary_crossentropy',\n",
    "        optimizer = 'adam',\n",
    "        metrics = ['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "subset = np.random.uniform(0, 1, len(train_data)) < 1.1 #Percentage of data to be used to fit the nn\n",
    "y_sample = train[\"y\"][subset]\n",
    "y_sample.reset_index(drop = True, inplace = True)\n",
    "classifier = create_model(max_tokens, np.shape(we)[1])\n",
    "fit = classifier.fit(x = train_data[subset, :, :], y = y_sample, batch_size=4, epochs=2, verbose=1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD THE TEST DATA, USE THE MODEL TO PREDICT THE SENTIMENTS AND SAVE THE PREDICTIONS\n",
    "\n",
    "def load_and_prepare_test_data(data_path,vocab, we):\n",
    "    \"\"\"Loads data and returns ids (event ids) and X (feature vector)\"\"\"\n",
    "    f = open(data_path, encoding='utf-8')\n",
    "    test = f.read().splitlines()\n",
    "    tweets = []\n",
    "    ids = []\n",
    "    for line in test:\n",
    "        id, tweet = line.split(',',1)\n",
    "        tweets.append(tweet)\n",
    "        ids.append(id)\n",
    "    test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    test[\"w\"] = test[\"Tweets\"].apply(lambda x: representation(x, we, vocab))\n",
    "    col = [\"w\" + str(k) for k in range(np.shape(we)[1])]\n",
    "    test[col] = test[\"w\"].apply(pd.Series)\n",
    "    test.drop(\"w\", axis=1, inplace = True)\n",
    "    test.drop(\"Tweets\", axis=1, inplace = True)\n",
    "    \n",
    "    drops = test[test.isnull().any(axis=1)].index\n",
    "    #for the tweets which do not have any words in the cutted vocabulary, predict 1\n",
    "    test.fillna(1, inplace=True)\n",
    "    \n",
    "    return test,drops\n",
    "\n",
    "test, drops = load_and_prepare_test_data('Datasets/test_data.txt', vocab, we)\n",
    "test[\"Prediction\"] = clf.predict(test)\n",
    "#test[\"Id\"] = test.index\n",
    "test[\"Prediction\"].to_csv(\"Submissions/submission\" + time + \".csv\", header= True, index_label = \"Id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
