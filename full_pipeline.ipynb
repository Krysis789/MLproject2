{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT USED PACKAGES AND SET SEED\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import *\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pbs\n",
    "import os\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING DATASETS BEFORE PREPROCESSING\n",
    "\n",
    "#Define the used datasets here\n",
    "positive_dataset = 'Datasets/train_pos.txt'\n",
    "negative_dataset = 'Datasets/train_neg.txt'\n",
    "\n",
    "def load_in_pd(data_path):\n",
    "    with open(data_path, encoding=\"utf-8\") as f :\n",
    "        x = f.readlines()\n",
    "        x = pd.DataFrame(x,columns=['Tweets'])\n",
    "    return (x)\n",
    "\n",
    "positive_pd = load_in_pd(positive_dataset)\n",
    "negative_pd = load_in_pd(negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPLYING DESIRED PREPROCESSING\n",
    "\n",
    "#Define the desired preprocessing method in this function\n",
    "def preprocess(tweet_data):\n",
    "    #tweet_data.drop_duplicates(inplace=True)\n",
    "    return tweet_data\n",
    "\n",
    "positive_preprocessed = preprocess(positive_pd)\n",
    "negative_preprocessed = preprocess(negative_pd)\n",
    "\n",
    "time = datetime.strftime(datetime.now(), \"%Y_%m_%d_%H_%M_%S\")\n",
    "if not os.path.exists(\"Processed_data\"):\n",
    "    os.makedirs(\"Processed_data\")\n",
    "    \n",
    "with open('Processed_data/' + 'positive_preprocessed' + time + '.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in positive_preprocessed[\"Tweets\"]:\n",
    "        f.write(\"%s\" % tweet)\n",
    "        \n",
    "with open('Processed_data/' + 'negative_preprocessed' + time + '.txt', \"w+\", encoding = \"UTF-8\") as f:\n",
    "    for tweet in negative_preprocessed[\"Tweets\"]:\n",
    "        f.write(\"%s\" % tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##APPLY build_vocab.sh AND CUT THE VOCABULARY USING THE CHOSEN THRESHOLD\n",
    "\n",
    "#Choose the desired cutting parameter here (Tokens with >= cut_threshold occurrences are kept)\n",
    "cut_threshold = 5\n",
    "\n",
    "\n",
    "arg1 = 'positive_preprocessed' + time + '.txt'\n",
    "arg2 = 'negative_preprocessed' + time + '.txt'\n",
    "arg3 = 'vocab_' + time + '.txt'\n",
    "vocab_successful = os.system(\"build_vocab.sh \" + arg1 + \" \" + arg2 + \" \" + arg3)\n",
    "\n",
    "if (vocab_successful != 0):\n",
    "    sys.exit(\"Building vocabulary failed.\")\n",
    "    \n",
    "def cut_and_save_vocab(file_in, file_out):\n",
    "    full_vocab = pd.read_csv(file_in, sep = \"(\\s+)\", header=None, engine = 'python')\n",
    "    cutted_vocab = full_vocab[full_vocab[0] >= cut_threshold][2]\n",
    "    with open(file_out, 'w+') as f:\n",
    "        f.write(cutted_vocab.to_string(header = False, index = False))\n",
    "    \n",
    "cut_and_save_vocab('Processed_data/vocab_' + time + '.txt', 'Processed_data/vocab_cut' + time + '.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DUMP THE BUILT VOCABULARY TO A PICKLE FILE\n",
    "vocab = dict()\n",
    "with open('Processed_data/vocab_cut' + time + '.txt') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        vocab[line.strip()] = idx\n",
    "\n",
    "with open('Processed_data/vocab_' + time + '.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREATE A CO-OCCURRENCE MATRIX\n",
    "def create_cooc(vocab_file, negative_file, positive_file, output_file):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    vocab_size = len(vocab)\n",
    "    data, row, col = [], [], []\n",
    "    counter = 1\n",
    "    for fn in [negative_file, positive_file]:\n",
    "        with open(fn,encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "                tokens = [t for t in tokens if t >= 0]\n",
    "                for t in tokens:\n",
    "                    for t2 in tokens:\n",
    "                        data.append(1)\n",
    "                        row.append(t)\n",
    "                        col.append(t2)\n",
    "\n",
    "                if counter % 10000 == 0:\n",
    "                    print(counter)\n",
    "                counter += 1\n",
    "\n",
    "                if counter % 200000 == 0:\n",
    "                    print(len(data))\n",
    "                    cooc = coo_matrix((data, (row, col)))\n",
    "                    print(\"summing duplicates (this can take a while)\")\n",
    "                    cooc.sum_duplicates()\n",
    "                    data=list(cooc.data)\n",
    "                    row=list(cooc.row)\n",
    "                    col=list(cooc.col)\n",
    "                    print(len(data))\n",
    "\n",
    "    print(len(data))\n",
    "    cooc = coo_matrix((data, (row, col)))\n",
    "    print(\"summing duplicates (this can take a while)\")\n",
    "    cooc.sum_duplicates()\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(cooc, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "#Used datasets should be defined on the second code block\n",
    "create_cooc('Processed_data/vocab_' + time + '.pkl', 'Processed_data/negative_preprocessed' + time + '.txt', 'Processed_data/positive_preprocessed' + time + '.txt',\n",
    "            'Processed_data/cooc_pickle' + time + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPLY glove.py\n",
    "random.seed(123)\n",
    "def glove(cooc_pickle, output_file):\n",
    "    print(\"loading cooccurrence matrix\")\n",
    "    with open(cooc_pickle, 'rb') as f:\n",
    "        cooc = pickle.load(f)\n",
    "    print(\"{} nonzero entries\".format(cooc.nnz))\n",
    "\n",
    "    nmax = 100\n",
    "    print(\"using nmax =\", nmax, \", cooc.max() =\", cooc.max())\n",
    "\n",
    "    print(\"initializing embeddings\")\n",
    "    embedding_dim = 20\n",
    "    xs = np.random.normal(size=(cooc.shape[0], embedding_dim))\n",
    "    ys = np.random.normal(size=(cooc.shape[1], embedding_dim))\n",
    "\n",
    "    eta = 0.001\n",
    "    alpha = 3 / 4\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
    "            logn = np.log(n)\n",
    "            fn = min(1.0, (n / nmax) ** alpha)\n",
    "            x, y = xs[ix, :], ys[jy, :]\n",
    "            scale = 2 * eta * fn * (logn - np.dot(x, y))\n",
    "            xs[ix, :] += scale * y\n",
    "            ys[jy, :] += scale * x\n",
    "    np.save(output_file, xs)\n",
    "\n",
    "glove('Processed_data/cooc_pickle' + time + '.pkl', 'Processed_data/embeddings' + time + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PREPARE DATA FOR TRAINING A CLASSIFIER\n",
    "\n",
    "def load_train_data(data_path):\n",
    "    \"\"\"Loads data and returns ids (event ids) and X (feature vector)\"\"\"\n",
    "    f = open(data_path, encoding=\"utf-8\")\n",
    "    x = f.readlines()\n",
    "    x = pd.DataFrame(x, columns=['Tweets'])\n",
    "    return x\n",
    "\n",
    "def representation(tweet, we, vocab):\n",
    "    acc = np.array(0)\n",
    "    n_ignored_word = 0\n",
    "    for word in tweet.split():\n",
    "        if word not in vocab.keys():\n",
    "            n_ignored_word += 1\n",
    "        else:\n",
    "            try:\n",
    "                acc = np.add(acc,we[vocab[word]])\n",
    "            except: \n",
    "                #print(\"problem with \" + word) #last word from vocab is missing in cooc\n",
    "                n_ignored_word += 1\n",
    "    n = len(tweet.split()) - n_ignored_word\n",
    "    acc = acc / n\n",
    "    return(acc)\n",
    "\n",
    "\n",
    "def create_train_data(positive_path, negative_path, vocab, we):\n",
    "    pos = load_train_data(positive_path)\n",
    "    neg = load_train_data(negative_path)\n",
    "    pos[\"y\"] = 1\n",
    "    neg[\"y\"] = -1\n",
    "    train = pd.concat([pos, neg])\n",
    "    train.reset_index(drop = True, inplace = True)\n",
    "    train[\"w\"] = train[\"Tweets\"].apply(lambda x: representation(x, we, vocab))\n",
    "    train.drop(\"Tweets\", axis=1, inplace = True)\n",
    "    \n",
    "    col = [\"w\" + str(k) for k in range(np.shape(we)[1])]\n",
    "    train[col] = train[\"w\"].apply(pd.Series)\n",
    "    train.drop(\"w\",axis=1,inplace=True)\n",
    "    \n",
    "    #remove the tweets which do not have any words used more than 5 times in the training dataset\n",
    "    train.dropna(inplace=True) \n",
    "    \n",
    "    return train\n",
    "\n",
    "with open('Processed_data/vocab_' + time + '.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "we = np.load('Processed_data/embeddings' + time + '.npy')\n",
    "#The names of the datasets are defined in the second code block\n",
    "train = create_train_data(positive_dataset, negative_dataset, vocab, we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAIN A CLASSIFIER\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "X = train.drop(\"y\", axis=1)\n",
    "y = train[\"y\"]\n",
    "random.seed(123)\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD THE TEST DATA, USE THE MODEL TO PREDICT THE SENTIMENTS AND SAVE THE PREDICTIONS\n",
    "\n",
    "def load_and_prepare_test_data(data_path,vocab, we):\n",
    "    \"\"\"Loads data and returns ids (event ids) and X (feature vector)\"\"\"\n",
    "    f = open(data_path, encoding='utf-8')\n",
    "    test = f.readlines()\n",
    "    tweets = []\n",
    "    ids = []\n",
    "    for line in test:\n",
    "        id, tweet = line.split(',',1)\n",
    "        tweets.append(tweet)\n",
    "        ids.append(id)\n",
    "    test = pd.DataFrame(tweets,ids,columns=['Tweets'])\n",
    "    test[\"w\"] = test[\"Tweets\"].apply(lambda x: representation(x, we, vocab))\n",
    "    col = [\"w\" + str(k) for k in range(np.shape(we)[1])]\n",
    "    test[col] = test[\"w\"].apply(pd.Series)\n",
    "    test.drop(\"w\", axis=1, inplace = True)\n",
    "    test.drop(\"Tweets\", axis=1, inplace = True)\n",
    "    \n",
    "    drops = test[test.isnull().any(axis=1)].index\n",
    "    #for the tweets which do not have any words in the cutted vocabulary, predict 1\n",
    "    test.fillna(1, inplace=True)\n",
    "    \n",
    "    return test,drops\n",
    "\n",
    "test, drops = load_and_prepare_test_data('Datasets/test_data.txt', vocab, we)\n",
    "test[\"Prediction\"] = clf.predict(test)\n",
    "#test[\"Id\"] = test.index\n",
    "test[\"Prediction\"].to_csv(\"Submissions/submission\" + time + \".csv\", header= True, index_label = \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
